\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[usenames, dvipsnames]{color}
\usepackage[breaklinks=true]{hyperref}
\graphicspath{ {./} }
\setlength\parindent{0pt}

\title{Sparse Implementations of SGD-like algorithms}
\author{Jiahao Xie}
\begin{document}
\maketitle

\section{Introduction}

Consider the following strongly convex finite sum problem where each individual functions are convex and have Lipschitz continuous gradients
\begin{equation} \label{eq:problem}
\begin{aligned}
    \mathrm{min} f(x) = \frac{1}{n} \sum_{i=1}^{n} f_i (x),
\end{aligned}
\end{equation}
where $f_i(x) = \phi(A_i^T x)$, and $A_i$ represents the $i_{th}$ sample.

SGD-like algorithms use to following abstract update rule at each iteration
\begin{equation} \label{eq:sgd}
\begin{aligned}
    x_{t+1} = x_{t} - \gamma g_{t},
\end{aligned}
\end{equation}
where $\gamma$ represents the step size and $g_{t}$ stands for approximation of the full gradient $\nabla f(x_t)$. For example, SAGA uses the following $g_{t}$ to approximate the full gradient:
\begin{equation} \label{eq:saga}
\begin{aligned}
    \mathrm{SAGA: } &g_{t} = \nabla f_i(x_t) - \alpha_i + \bar{\alpha},
\end{aligned}
\end{equation}
where $\alpha_i$ stands for the latest gradient evaluated before iteration $t$ for sample $i$, and $\bar{\alpha} = 1/n \sum_{i=1}^n \alpha_i$. Thus the update rule of SAGA becomes
\begin{equation} \label{eq:saga_rule}
\begin{aligned}
    x_{t+1} = x_{t} - \gamma \big( \nabla f_i(x_t) - \alpha_i + \bar{\alpha} \big).
\end{aligned}
\end{equation}


\section{Sparse Implementation}
The general update rule \eqref{eq:sgd} requires full vector operations, which is unfavorable when the dataset is spare. Fortunately, we can leverage sparsity to reduce computational cost for solving problem \eqref{eq:problem}. In this section, we introduce three ways (lazy update, decomposed update, and sparse update) to implement SGD-like algorithms on sparse datasets for solving problem \eqref{eq:problem}. We illustrate these methods using SAGA algorithm as an example, but they also apply to other SGD-like algorithms, such as SGD, SAG, and SVRG.

\subsection{Lazy Update}
The lazy update method is proposed by \cite{schmidt2017}. Since $\nabla f_i(x) = \phi'(A_i^T x) A_i$, one can see that the term $\gamma (\nabla f_i(x_t) - \alpha_i)$ is sparse. And the update to $\bar{\alpha}$ ($\bar{\alpha} \leftarrow \bar{\alpha} + \frac{\nabla f_i(x_t) - \alpha_i}{n}$) is also a sparse operation. Let $S_i$ denotes the support of sample $i$, and $[\cdot]_v$ denotes the $v_{th}$ coordinate of a vector, one can observe that $[\bar{\alpha}]_v$ remanis unchanged at iteration $t$ if $v \notin S_i$.

\bigbreak

Based on these observations, we may defer
%updates of those unchanged coordinates until we need to access those coordinates at some iteration.
the dense part ($\bar{\alpha}$) of the update \eqref{eq:saga_rule} and only perform sparse operations at each iteration.
To do so, we need to keep a counter $c_v$ for each coordinate $v$ to record the last time it is visited, which requires $\mathcal{O}(d)$ extra storage where $d$ represents the dimension of data. At the start of each iteration, we perform the lazy update $[x]_v \leftarrow [x]v + (t - c_v) \gamma [\bar{\alpha}]_v$ to obtain the accurate $[x]_v$ for $v \in S_i$ just in time (so that we can compute the stochastic gradient).
%, which will be used to compute the stochastic gradient $\nabla f_i(x)$.

\subsection{Decomposed Update}
Another way to avoid dense vector operations at each iteration is to decompose the iterate $x$ as a combination of two or more dense vectors and perform sparse operations to those vectors. For SAGA algorithm, we can rewrite $x_t$ as
\begin{equation}
\begin{aligned}
    x_t = c_t^1 \hat{x}_t + c_t^2 \hat{\alpha}_t.
\end{aligned}
\end{equation}

Then we have
\begin{equation}
\begin{aligned}
    x_{t+1} &= x_t - \gamma (\nabla f_i(x_t) - \alpha_i + \bar{\alpha})  \\
            &= c_t^1 \big[ \hat{x}_t - \gamma (\nabla f_i(x_t) - \alpha_i) \big] + (c_t^2 - \gamma) \big[ \hat{\alpha}_t - \gamma (\bar{\alpha} - \hat{\alpha}_t) / (c_t^2 - \gamma) \big] \\
            &= c_{t+1}^1 \hat{x}_{t+1} + c_{t+1}^2 \hat{\alpha}_{t+1},
\end{aligned}
\end{equation}
where $c_{t+1} = c_{t}$, $\hat{x}_{t+1} = \hat{x}_t - \gamma (\nabla f_i(x_t) - \alpha_i)$, $c_{t+1}^2 = c_t^2 - \gamma$, and $\hat{\alpha}_{t+1} = \hat{\alpha}_t - \gamma (\bar{\alpha} - \hat{\alpha}_t) / (c_t^2 - \gamma)$.

\bigbreak

Since the update to $\hat{x_t}$ and
$\hat{\alpha_t}$ % @NOTE \bar{\alpha} - \hat{\alpha} might not be sparse ???
are sparse, we only need to perform sparse operations on them at each iteration, as well as update of scalars $c_t^1$ and $c_t^2$.


\subsection{Sparse Update}
We note that the above two methods does not change the algorithm, that is, the algorithm implemented by lazy update or decomposed update strategy is equivalent to the original algorithm. However the third method, sparse update, which we are going to introduce in this section, does change the algorithm. The sparse update method is proposed by \cite{mania2015}. Its main idea is to replace the dense vector $\bar{\alpha}$ in \eqref{eq:saga} with a sparse equivalent in expectation, $D_i \bar{\alpha}$, i.e., $\mathbb{E} D_i \bar{\alpha} = \bar{\alpha}$. Here $Di = P_{S_i} D$, where $P_{S_i}$ is a projection matrix that projects a vector onto the support $S_i$, and $D$ is a  $d \times d$ diagonal matrix $\mathrm{diag}\{1/p_1, \dots, 1/p_d\}$, where $p_v$ denotes the probability that $[A_i]_v$ is nonzero ($i=1, \dots, n$).

\section{Problems with Katyusha}
The Katyusha algorithm \cite{allen2016} is a direct acceleration of stochastic gradient methods. Katyusha uses the following update rule for problem \eqref{eq:problem}:
\begin{equation} \label{eq:katyusha}
\begin{aligned}
    x_{t+1} &= \tau_1 z_t + \tau_2 \tilde{x} + \tau_3 y_k \\
    \tilde{\nabla}_{t+1} &= \nabla f(\tilde{x}) + \nabla f_i(x_{t+1}) - \nabla f_i(\tilde{x}) \\
    y_{t+1} &= x_{t+1} - \frac{1}{3L} \tilde{\nabla}_{t+1} \\
    z_{t+1} &= z_{t} - \alpha \tilde{\nabla}_{t+1},
\end{aligned}
\end{equation}
where $\tau_1 + \tau_2 + \tau_3 = 1$ and $\tau_2 = 1/2$, $L$ is the Lipschitz constant of the problem, $\alpha$ is a constant, and $\tilde{x}$ is a snapshot of $x$ which is updated at the start of each epoch like that of SVRG algorithm.

We can rewrite the update rule of Katyusha using the decomposed update method as follows:
\begin{equation}
\begin{aligned}
    x_t &= b_t^1 \hat{x_t} + b_t^2 \tilde{x} + b_t^3 \nabla f(\tilde{x}) + b_t^4 \hat{z_t} \\
    z_t &= a_t^1 \hat{z_t} + a_t^2 \nabla f(\tilde{x}),
\end{aligned}
\end{equation}
where
\begin{equation}
\begin{aligned}
    a_{t+1}^1 &= a_t^1, \  a_0^1 = 1 \\
    a_{t+1}^2 &= a_t^2 - \alpha,\  a_0^2 = 0  \\
    \hat{z}_{t+1} &= \hat{z}_t - \frac{\alpha \big(\nabla f_i(x_{t+1}) - \nabla f_i(\tilde{x}) \big)}{a_t^1}  \\
    b_{t+1}^1 &= \tau_3 b_t^1,\  b_0^1 = 1 \\
    b_{t+1}^2 &= \tau_3 b_t^2 + \tau_2,\  b_0^2 = 0 \\
    b_{t+1}^3 &= \tau_3 b_t^3 - \farc{\tau_3}{3L} + \tau_1 \tau_3 a_t^2,\  b_0^3 = 0 \\
    b_{t+1}^4 &= \tau_1 \tau_3 a_t^1 + \tau_3 b_t^4,\  b_0^4 = 0\\
    \hat{x}_{t+1} &= \hat{x}_t + \frac{\big(\nabla f_i(x_{t+1}) - \nabla f_i(\tilde{x}) \big)}{\tau_3 b_t^1}.
\end{aligned}
\end{equation}

Since $\tau_3 < 1/2$, $b_t^1 = {\tau_3}^t$ will become too small (and 1/$b_t^1$ will become too large) after several hundreds of iterations (e.g., ${\tau_3}^{400} \le 10^{-120}$). Thus we have to renormalize $b_t^1$ periodically, say every 400 iterations, before the factor becomes too small or too large. When the dataset is large, such renormalization is costly since it requires expansive dense vector operations and has to be done frequently. The other two spare implementations (lazy update and sparse update) have exactly the same problem for Katyusha. As a result, when compared with SVRG algorithm, Katyusha might takes more time than SVRG to attain a certain accuracy for sparse datasets.


\bibliographystyle{apalike}
\bibliography{cite}
\end{document}
